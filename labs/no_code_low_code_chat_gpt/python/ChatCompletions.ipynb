{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "### 1. [Introduction](#Introduction)\n",
    "### 2. [Set Up Python Virtual Environment (venv), Dependencies, and Jupyter Instance](#Set-Up-Python-Virtual-Environment-(venv),-Dependencies,-and-Jupyter-Instance)\n",
    "### 3. [Overview of the Chat Completion API](#Overview-of-the-Chat-Completion-API)\n",
    "### 4. [Example ChatCompletion.create() Calls](#Example-ChatCompletion.create()-Calls)\n",
    "### 5. [Creating and Managing a Basic Conversation Loop](#Creating-and-Managing-a-Basic-Conversation-Loop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ChatGPT and GPT-4 models are optimized for conversational interfaces and work differently than the older GPT-3 models. They are conversation-in and message-out, and require input formatted in a specific chat-like transcript format. Azure OpenAI provides two different options for interacting with these models: Chat Completion API and Completion API with Chat Markup Language (ChatML). \n",
    "The Chat Completion API is the preferred method for accessing these models, while ChatML provides lower level access but requires additional input validation and only supports ChatGPT models. It's important to use the techniques described in the article to get the best results from the new models.\n",
    "\n",
    "This notebook will cover the aspects of the Chat Completion Python API with conversation, roles (system, assistant, user) and examples of different usage scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Python Virtual Environment (venv), Dependencies, and Jupyter Instance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Python Virtual Environment (venv)\n",
    "\n",
    "**Set up your environment (You can skip this if you have a working environment)**\n",
    "**These commands should be entered in the command line or console**\n",
    "\n",
    "**Instructions as written are for command prompt within VS Code**\n",
    "\n",
    "**Instructions were written for Python 3.10.4**\n",
    "\n",
    "**Create virtual environment to start - the first portion of the code below**\n",
    "```commandline\n",
    "C:\\<Location>\\<To your python install>\\python -m venv C:\\<Location>\\<You want to create your virtual environment>\n",
    "```\n",
    "\n",
    "**Change directory to new virtual environment and activate**\n",
    "```commandline\n",
    "cd C:\\<Location>\\<You created your virtual environment in>\\scripts\n",
    "\n",
    "Activate\n",
    "```\n",
    "\n",
    "**Change directory to code repository**\n",
    "```commandline\n",
    "cd C:\\<Location>\\<To your code repository>\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install dependencies\n",
    "\n",
    "In the command line, install the following packages via pip install\n",
    "\n",
    "```commandline\n",
    " pip install <package name>\n",
    " ```\n",
    "\n",
    "* openai\n",
    "* jupyterlab\n",
    "* tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch Jupyter instance\n",
    "\n",
    "Launch your Jupyter instance by starting the jupyter server:\n",
    "\n",
    "```commandline\n",
    "jupyter-lab\n",
    "```\n",
    "\n",
    "You may need to select your virtual environment as the kernel for your notebook - this can be done in the top right corner of your VS Code Instance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Environment Variables\n",
    "\n",
    "Add in the environment variables for your OpenAI API Key and API Base URL. You can find these values in the Azure Portal under your OpenAI resource.\n",
    "\n",
    "We can add in new variables as such:\n",
    "Add in new environment variables\n",
    "os.environ['NEW_VARIABLE_NAME'] = '/new/value' as shown in the code below\n",
    "\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '<YOUR OPENAI API KEY'\n",
    "os.environ['OPENAI_API_BASE'] = '<YOUR OPENAI API ENDPOINT URL>'\n",
    "os.environ['OPEN_AI_ENGINE'] = '<NAME UNDER WHICH YOU DEPLOYED YOUR MODEL>'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your environment variables here or hard-code them (not recommended)\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '<YOUR OPENAI API KEY'\n",
    "os.environ['OPENAI_API_BASE'] = '<YOUR OPENAI API ENDPOINT URL>'\n",
    "os.environ['OPEN_AI_ENGINE'] = '<NAME UNDER WHICH YOU DEPLOYED YOUR MODEL>'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call os.getenv() to retrieve the value of an environment variable. If the variable does not exist, os.getenv() returns None. In that case you may need to verify\n",
    "that you are using the same name as the environment variable you created or try re-running the os.environ command.\n",
    "\n",
    "```python\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\" \n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Your Azure OpenAI resource's key value.\n",
    "openai_engine = os.getenv(\"OPEN_AI_ENGINE\") # The name you gave your OpenAI Model deployment in Azure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our 3 required libraries\n",
    "import os\n",
    "import openai\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our Key, Endpoint, and deployed model name from our environment variables\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\" \n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")  # Your Azure OpenAI resource's endpoint value.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Your Azure OpenAI resource's key value.\n",
    "openai_engine = os.getenv(\"OPEN_AI_ENGINE\") # The name you gave your OpenAI Model deployment in Azure\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Chat Completion API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The following parameters aren't available with the new ChatGPT and GPT-4 models: **logprobs**, **best_of**, and **echo**. If you set any of these parameters, you'll get an error. gpt-35-turbo is equivalent to the gpt-3.5-turbo model from OpenAI.\n",
    "\n",
    "##### ChatCompletion.create()\n",
    "OpenAI trained the ChatGPT and GPT-4 models to accept input formatted as a conversation. The messages parameter takes an array of dictionaries with a conversation organized by role. The three types of roles are:\n",
    "\n",
    "* system\n",
    "* assistant\n",
    "* user \n",
    "\n",
    "A sample input containing a simple system message, a one-shot example of a user and assistant interacting, and the final \"actual\" user-supplied prompt is shown below:\n",
    "\n",
    "```json\n",
    "{\"role\": \"system\", \"content\": \"Provide some context and/or instructions to the model.\"},\n",
    "{\"role\": \"user\", \"content\": \"Example question goes here.\"}\n",
    "{\"role\": \"assistant\", \"content\": \"Example answer goes here.\"}\n",
    "{\"role\": \"user\", \"content\": \"First question/message for the model to actually respond to.\"}\n",
    "```\n",
    "\n",
    "Let's dive deeper into our 3 possible roles types of system, user, and assistant.\n",
    "\n",
    "##### System Role\n",
    "The system role, also known as the system message, is included at the beginning of the array. This message provides the initial instructions to the model. You can provide various information in the system role including:\n",
    "\n",
    "* A brief description of the assistant\n",
    "* Personality traits of the assistant\n",
    "* Instructions or rules you would like the assistant to follow\n",
    "* Data or information needed for the model, such as relevant questions from an FAQ\n",
    "\n",
    "You can customize the system role for your use case or just include basic instructions. The system role/message is optional, but it's recommended to at least include a basic one to get the best results.\n",
    "\n",
    "##### Assistant Role\n",
    "\n",
    "The assistant role is that of OpenAI or your assistant. You can omit this role in an intial ChatCompletion.create() call if desired, though it is required if you are going to pass a one-shot or few-shot example through the messages parameter. \n",
    "\n",
    "Let's take a look at some examples of the Chat Completion API in action.\n",
    "\n",
    "##### User Role\n",
    "\n",
    "The user role is the message that the user sends to the assistant. This is the message that the model will respond to. The user role is required for the model to respond.\n",
    "\n",
    "> **Note:** To trigger a response from the model, you should end with a user message indicating that it's the assistant's turn to respond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the max_tokens parameter has been set to 500 - this is not requried but suggested that you increase the max tokens to allow for longer responses\n",
    "# so they do not get cut-off mid-response.\n",
    "\n",
    "# openai_engine parameter is the name supplied to the deployed OpenAI model (This is something that is set at the time the model is deployed)\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=openai_engine, # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the difference between garbanzo beans and chickpeas?\"},\n",
    "    ],\n",
    "    # temperature=0.75,\n",
    "    # max_tokens=500,\n",
    "    # top_p=0.90,\n",
    "    # frequency_penalty=0,\n",
    "    # presence_penalty=0,\n",
    "    # stop=None\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we supplied two of our roles: system and user. The system message is very simple - it just provides some context for the model. The user message is the prompt that the model will respond to. Let's see what the model returns.\n",
    "\n",
    "> **Note:** The cell above has some unused parameters commented out. These are not required for the ChatCompletion.create() call to run, but they are available if you want to use them. Please feel free to experiment with these parameters through-out the notebook to see how they affect the model's output. It is typically best practice to adjust either temperature or top_p, but not both, as they have an interaction effect around the determinism of the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our print(response)'s output above we can see we get an array of responses. We'll mainly focus on the \"content\" within the \"message\" returned under \"choices\". However, it may be helpful to understand a few of the other views, such as \"finish_reason\". Every response includes a finish_reason. The possible values for finish_reason are:\n",
    "\n",
    "* **stop**: API returned complete model output.\n",
    "* **length**: Incomplete model output due to max_tokens parameter or token limit.\n",
    "* **content_filter**: Omitted content due to a flag from our content filters.\n",
    "* **null**:API response still in progress or incomplete.\n",
    "\n",
    "The material supplied under \"usage\" can also be helpful when trying to keep track of the number of tokens used in your request. The \"usage\" object includes the following information:\n",
    "* **completion_tokens**: The number of tokens used to complete the prompt - this should typically be from the \"assistant\" role.\n",
    "* **prompt_tokens**: The number of tokens used to prompt the model - this should typically be from the \"user\" role.\n",
    "* **total_tokens**: The total number of tokens used in the request.\n",
    "\n",
    "However, we're most interested in the \"content\" within the \"message\" returned under \"choices\". Let's take a look at the \"content\" returned in the response above by printing out just the assistant's response by accessing the \"content\" within the \"message\" returned under \"choices\":\n",
    "\n",
    "Copy and paste this code into the cell below and run it to see the assistant's response:\n",
    "```python\n",
    "print(response['choices'][0]['message']['content'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our print(response['choices'][0]['message']['content'])'s output above we can see that the \"assistant\" responded by informing the user that both Garbanzo and chickpeas refer to the same. In the next sections, we'll focus on how we can refine our ChatCompletions.create() calls to get more specific responses or fit different scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example ChatCompletion.create() Calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic system message previously was:\n",
    "```json\n",
    "{\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"}\n",
    "```\n",
    "\n",
    "This gives us an assistant who approximates the initial OpenAI ChatGPT assistant. Let's see what happens if we change the system message to:\n",
    "```json\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is an intelligent chatbot designed to help users answer their tax related questions.\\\n",
    "        Instructions:\\\n",
    "        - Only answer questions related to taxes.\\\n",
    "        - If you are unsure of an answer, you can say 'I do not know' or 'I am not sure' and recommend users go to the IRS website for more information. \"}, # Paste the system role message from above between the two curly braces\n",
    "        {\"role\": \"user\", \"content\": \"When are my taxes due?\"}\n",
    "```\n",
    "\n",
    "To create an assistant to help us file our taxes.\n",
    "\n",
    "You can copy and paste the above system role message into the cell below and run it to see the assistant's response:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    engine=openai_engine, # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    messages=[\n",
    "        {PASTE CONTENT FROM ABOVE HERE} # Paste in the role messages from above,\n",
    "    ],\n",
    "    temperature=0.50,\n",
    "    max_tokens=500,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the answer above that our assistant got the *typical* date when federal taxes are due correct - April 15th and notes that deadlines were extended during the COVID-19 Pandemic.\n",
    "However, in 2023, April 15th happens to fall on a Saturday, so the deadline has been pushed back to April 18th. Let's see if we can get our assistant to respond with the correct date.\n",
    "\n",
    "To do so, let's provide our assistant the correct answer and reasoning in a few-shot example. We'll add the following to our messages call in ChatCompletion.create():\n",
    "```json\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is an intelligent chatbot designed to help users answer their tax related questions. \"},\n",
    "        {\"role\": \"user\", \"content\": \"When do I need to file my taxes by?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"In 2023, you will need to file your taxes by April 18th. The date falls after the usual April 15th deadline because April 15th falls on a Saturday in 2023. For more details, see https://www.irs.gov/filing/individuals/when-to-file.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How can I check the status of my tax refund?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"You can check the status of your tax refund by visiting https://www.irs.gov/refunds\"}\n",
    "```\n",
    "\n",
    "Here what we are doing is providing our assistant with a few-shot example of a user asking about the tax deadline and our assistant providing the correct answer. We are also providing our assistant with an example of how to respond if a user requests information about their tax refund and the URL to the IRS website where they can check the status of their refund.\n",
    "\n",
    "Copy and paste the role messages above into the cell below and run to verify that the assistant's response is now correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    engine=openai_engine, # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    messages=[\n",
    "        {PASTE CONTENT FROM ABOVE HERE} # Paste in the role messages from above, keeping the new user message below\n",
    "        {\"role\": \"user\", \"content\": \"When do I need to file my taxes by?\"},\n",
    "    ],\n",
    "    temperature=0.50,\n",
    "    max_tokens=500,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our assistant now responds with the correct date for the 2023 tax deadline!\n",
    "\n",
    "In the example above, we were able to supply the correct answer to a common question via few shot learning; this approach is similar to GPT-3 models, but the format is slightly different with the back-and-forth-format that ChatCompletion entails. This approach can also be useful in demonstrating a behavior pattern to the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expanding the system role message to include more context\n",
    "\n",
    "Another way we can utilize the ChatCompletion format to interact and adjust our assistant is to supply it with a specific context. As with all promot engineering, the best practicies of being concise, clear, specific, and affirmative (telling the model what to do as opposed what not to do apply.)\n",
    "\n",
    "Let's see what happens if we provide our assistant with some context about Azure OpenAI Service.\n",
    "\n",
    "Copy and paste the following system role message into the cell below and run it to see the assistant's response:\n",
    "\n",
    "```json\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is an intelligent chatbot designed to help users answer technical questions about Azure OpenAI Serivce.\\\n",
    "            Only answer questions using the context below and if you're not sure of an answer, you can say 'I don't know'.\\\n",
    "            Context:\\\n",
    "            - Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the GPT-3, Codex and Embeddings model series.\\\n",
    "            - Azure OpenAI Service gives customers advanced language AI with OpenAI GPT-3, Codex, and DALL-E models with the security and enterprise promise of Azure.\\\n",
    "                 Azure OpenAI co-develops the APIs with OpenAI, ensuring compatibility and a smooth transition from one to the other.\\\n",
    "            - At Microsoft, we're committed to the advancement of AI driven by principles that put people first. Microsoft has made significant investments to help guard against abuse and unintended harm, which includes requiring applicants to show well-defined use cases, incorporating Microsoftâ€™s principles for responsible AI use.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is Azure OpenAI Service?\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    engine=openai_engine, # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    messages=[\n",
    "        {PASTE CONTENT FROM ABOVE HERE} # Paste in the role messages from above\n",
    "    ],\n",
    "    temperature=0.50,\n",
    "    max_tokens=500,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChatCompletion for non-chat based tasks\n",
    "\n",
    "You may wish to take advantage of some of the features that the GPT-3.5 and GPT-4 models offer in scenarios that are not strictly chat-based i.e. task-based scenarios. In many cases, you can supplant what you may have provided as part of a longer **prompt** in the GPT-3 models with a similar message in the **system role** of the ChatCompletion.create() call.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you may wish to utilize the model to extract information from unstructured data and return it in a more structured format. In this example, we may wish to take a voice message and retrieve the relevant information from it. We can do this by providing the model with the voice message and a system role message that describes the task we wish to accomplish:\n",
    "\n",
    "```json\n",
    "      {\"role\": \"system\", \"content\": '''\n",
    "      You are an assistant designed to extract entities from text.\n",
    "      Users will paste in a string of text and you will respond with entities you've extracted from the text as a JSON object.\n",
    "      Only respond with the JSON output.\n",
    "      Here's an example of your output format:\n",
    "      {\n",
    "      \"name\": \"\",\n",
    "      \"company\": \"\",\n",
    "      \"phone_number\": \"\"\n",
    "        }\n",
    "        '''},\n",
    "      {\"role\": \"user\", \"content\": \"Hello. My name is Robert Smith. I'm calling from Contoso Insurance, Delaware. My colleague mentioned that you are interested in learning about our comprehensive benefits policy. Could you give me a call back at (555) 346-9322 when you get a chance so we can go over the benefits?\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    engine=openai_engine, # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "    messages=[\n",
    "        {PASTE CONTENT FROM ABOVE HERE} # Paste in the role messages from above\n",
    "    ],\n",
    "    temperature=0.50,\n",
    "    max_tokens=500,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Managing a Basic Conversation Loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples so far have shown you the basic mechanics of interacting with the Chat Completion API. This example shows you how to create a conversation loop that performs the following actions:\n",
    "\n",
    "* Continuously takes console input, and properly formats it as part of the messages array as user role content.\n",
    "* Outputs responses that are printed to the console and formatted and added to the messages array as assistant role content.\n",
    "\n",
    "This means that every time a new question is asked, a running transcript of the conversation so far is sent along with the latest question. Since the model has no memory, you need to send an updated transcript with each new question or the model will lose context of the previous questions and answers.\n",
    "\n",
    "> **Note**: If you are using VS Code the window in which to enter your text will open at the top of the editor. You may need to press enter twice - once to enter the user input and again to generate the assistant role message. Once generated both message lines will be printed below the cell block as normal cell output. You may need to interrupt your kernel and restart it to end the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "while(True):\n",
    "    user_input = input()      \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=openai_engine, # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = conversation\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response['choices'][0]['message']['content']})\n",
    "    print(\"\\n\" + response['choices'][0]['message']['content'] + \"\\n\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Managing conversations\n",
    "\n",
    "The previous example will run until you hit the model's token limit. With each question asked, and answer received, the messages array grows in size. The token limit for gpt-35-turbo is 4096 tokens, whereas the token limits for gpt-4 and gpt-4-32k are 8192 and 32768 respectively. These limits include the token count from both the message array sent and the model response. The number of tokens in the messages array combined with the value of the max_tokens parameter must stay under these limits or you'll receive an error.\n",
    "\n",
    "It's your responsibility to ensure the prompt and completion falls within the token limit. This means that for longer conversations, you need to keep track of the token count and only send the model a prompt that falls within the limit.\n",
    "\n",
    "The following code sample shows a simple chat loop example with a technique for handling a 4096 token count using OpenAI's tiktoken library.\n",
    "\n",
    "The code requires **tiktoken 0.3.0.** If you have an older version run **pip install tiktoken --upgrade**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some of our variables for us in our chatbot\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "max_response_tokens = 250\n",
    "token_limit= 4096\n",
    "conversation=[]\n",
    "conversation.append(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"\n",
    "    Returns the number of tokens required to encode the given messages as a running\n",
    "    total of tokens. \n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while(True):\n",
    "    user_input = input(\"\")     \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "\n",
    "    while (conv_history_tokens+max_response_tokens >= token_limit):\n",
    "        del conversation[1] \n",
    "        conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "        \n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=openai_engine, # The deployment name you chose when you deployed the ChatGPT or GPT-4 model.\n",
    "        messages = conversation,\n",
    "        temperature=0.7,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response['choices'][0]['message']['content']})\n",
    "    print(\"You Entered: \" + \"\\n\" + user_input + \"\\n\")\n",
    "    print(\"Your Assistant Responded: \" + \"\\n\" + response['choices'][0]['message']['content'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
